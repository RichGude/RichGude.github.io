{"cells":[{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["\n","# Define figure constants:\n","mpl.rcParams['figure.figsize'] = (8, 6)\n","mpl.rcParams['axes.grid'] = False\n","\n","# Define other constants:\n","# set data working directory\n","dwd = os.path.join(os.getcwd(), 'EconData')\n","metal_list = ['Aluminum', 'Copper', 'IronOre',\n","              'Nickel', 'Zinc']      # set list of metal names\n","var_names = ['D12', 'E12', 'b/m', 'tbl', 'AAA', 'BAA', 'lty',\n","             'ntis', 'Rfree', 'infl', 'ltr', 'corpr', 'svar', 'SPvw']\n","\n","# %# Load data and begin data preparation:\n","econData = pd.read_excel(os.path.join(dwd, 'PredictorData2019.xlsx'), sheet_name='Monthly',\n","                         usecols=['Date'] + var_names, index_col=0)\n","priceData = pd.read_excel(os.path.join(\n","    dwd, 'PriceData.xlsx'), sheet_name='1990Price', index_col=0)\n","\n","# Review Data\n","# print(\"Economic Indicators:\\n', econData.head())\n","# print(\"Commodity Price Values:\\n', priceData.head())\n","# showCase = econData.plot(subplots=True)\n","# plt.show()\n","# All data looks acceptable and is otherwise able to proceed:\n","\n","# %# Train/Test/Validate Split\n","# data_length = len(econData)\n","# train_econ = econData[0:int(data_length*0.7)]\n","# valid_econ = econData[int(data_length*0.7):int(data_length*0.9)]\n","# test_econ = econData[int(data_length*0.9):]\n","#\n","# train_price = priceData[0:int(data_length*0.7)]\n","# valid_price = priceData[int(data_length*0.7):int(data_length*0.9)]\n","# test_price = priceData[int(data_length*0.9):]\n","#\n","# # Normalize Economic data\n","# econ_mean = train_econ.mean()\n","# econ_stdv = train_econ.std()\n","# train_econ = (train_econ - econ_mean)/econ_stdv\n","# valid_econ = (valid_econ - econ_mean)/econ_stdv\n","# test_econ = (test_econ - econ_mean)/econ_stdv\n","#\n","# # Normalize Price data\n","# price_mean = train_price.mean()\n","# price_stdv = train_price.std()\n","# train_price = (train_price - price_mean)/price_stdv\n","# valid_price = (valid_price - price_mean)/price_stdv\n","# test_price = (test_price - price_mean)/price_stdv\n","\n","# Review normalized data structure in a violin plot\n","# sample_std = (econData - econData.mean()) / econData.std()      # Define a new variable so as to not adjust current\n","# # Create a reverse-pivot table basically from a two-column dataframe for each violin-chart creation\n","# sample_std = sample_std.melt(var_name='Variable', value_name='Normalized')\n","# plt.figure(figsize=(14, 8))\n","# ax = sns.violinplot(x='Variable', y='Normalized', data=sample_std)\n","# _ = ax.set_xticklabels(econData.keys(), rotation=90)\n","# plt.show()\n","# All data looks acceptable and is otherwise able to proceed (despite large outliers on 'svar' and 'infl')\n","\n","# Create a window for reviewing past economic variable data for predicting current commodity prices\n","'''\n","In some cases, economic data may have a absolute and immediate impact on price data, such as inflation, discussed in the\n","Time Series section and quantitatively, but not qualitatively excluded from the real price data here, or treasury bill\n","rates, which are published by the government and can be immediately assessed for their return on investment over various\n","other investment opportunities.  In other cases, this data may have a delayed effect on price data, such as corporate\n","bond return or stock variance data being compiled and released for investor consumption and integration into buying and\n","selling behavior at a later date from their real-world calculation.\n","\n","For this delayed consumption effect reason, a six-month window going back in time from the current day will be used to\n","predict commodity price for the next month  (e.g., using economic data from January through June, calendar months 1\n","through 6, to predict commodity prices in July, calendar month 7).\n","\n","Define a class that takes in economic factors and price data dataframes\n","'''\n","\n","# Define a class that takes in economic factors and price data dataframes\n","\n","\n","class SampleGenerator:\n","    def __init__(self, metal_label, input_width=6, label_width=1, shift=1,\n","                 econ_data=econData, comm_data=priceData):\n","\n","        # Concatenate the metal label into the economic data to make one dataset from which to pull data\n","        self.data = pd.concat([econ_data, comm_data[metal_label]], axis=1)\n","\n","        # Split (70:20:10, train/validation/test):\n","        self.data_length = len(self.data)\n","        self.trn_data = self.data[0:int(self.data_length * 0.7)]\n","        self.val_data = self.data[int(\n","            self.data_length * 0.7):int(self.data_length * 0.9)]\n","        self.tst_data = self.data[int(self.data_length * 0.9):]\n","\n","        # Normalize the data:\n","        # Must use mean and standard deviation of training data, for appropriate rigor\n","        self.data_mean = self.trn_data.mean()\n","        self.data_stdv = self.trn_data.std()\n","        self.trn_data = (self.trn_data - self.data_mean) / self.data_stdv\n","        self.val_data = (self.val_data - self.data_mean) / self.data_stdv\n","        self.tst_data = (self.tst_data - self.data_mean) / self.data_stdv\n","\n","        # Work out the label column indices.\n","        # metal_label but be a list with string name(s) of metals(s) from list\n","        self.metal_label = metal_label\n","        self.column_indices = {name: i for i, name in\n","                               enumerate(self.trn_data.columns)}\n","\n","        # Work out the window parameters (input and label widths are\n","        # standard is 6 (i.e., 6 months back of information)\n","        self.input_width = input_width\n","        # standard is 1 (i.e., 1 month of prediction)\n","        self.label_width = label_width\n","        # standard is 1 (i.e., 1 month forward in prediction)\n","        self.shift = shift\n","\n","        # standard is 6 back + 1 forward = *7*\n","        self.total_window_size = self.input_width + self.shift\n","\n","        # standard is 'slice(0, 6, None)'\n","        self.input_slice = slice(0, self.input_width)\n","        self.input_indices = np.arange(self.total_window_size)[\n","            self.input_slice]    # std is 'array([0, 1, 2, 3, 4, 5])'\n","\n","        self.label_start = self.total_window_size - \\\n","            self.label_width                # standard is 7 - 1 = *6*\n","        # standard is 'slice(6, None, None)'\n","        self.labels_slice = slice(self.label_start, None)\n","        self.label_indices = np.arange(self.total_window_size)[\n","            self.labels_slice]   # standard is 'array([6])'\n","\n","    # Define output for self-calling a SampleGenerator object\n","    def __repr__(self):\n","        return '\\n'.join([\n","            f'Total window size: {self.total_window_size}',\n","            f'Input indices: {self.input_indices}',\n","            f'Label indices: {self.label_indices}',\n","            f'Label column name: {self.metal_label}'])\n","\n","    # SampleGenerator instance has a single object with all feature and label data.  Create a function, 'split_window',\n","    #   to separate single instance into two objects of features and labels over the same time frame\n","    def split_window(self, features):\n","        inputs = features[:, self.input_slice, :]\n","        labels = features[:, self.labels_slice, :]\n","        labels = tf.stack([labels[:, :, self.column_indices[name]]\n","                           for name in self.metal_label], axis=-1)\n","\n","        # Slicing doesn't preserve static shape information, so set the shapes\n","        # manually. This way the 'tf.data.Datasets' are easier to inspect.\n","        inputs.set_shape([None, self.input_width, None])\n","        labels.set_shape([None, self.label_width, None])\n","\n","        return inputs, labels\n","\n","    # Using a time-series DataFrame object, convert to TensorFlow data.Dataset object in feature and label window pairs\n","    def make_dataset(self, data, batch=6):\n","        data = np.array(data, dtype=np.float32)\n","        ds = tf.keras.preprocessing.timeseries_dataset_from_array(\n","            # numpy array containing consecutive-time data points\n","            data=data,\n","            targets=None,                               # set to 'None' to only yield input data\n","            # number of time steps in output sequence (std is *7*)\n","            sequence_length=self.total_window_size,\n","            # How many time steps to skip between each batch\n","            sequence_stride=1,\n","            # shuffle output sequences to improve model rigor\n","            shuffle=True,\n","            batch_size=batch, )                         # set batch size of Dataset (std is *6*)\n","\n","        # Automatically separate data into feature and label sets\n","        ds = ds.map(self.split_window)\n","\n","        return ds\n","\n","    # Define property values for training, validating, and testing data\n","    @property\n","    def train(self):\n","        return self.make_dataset(self.trn_data)\n","\n","    @property\n","    def validate(self):\n","        return self.make_dataset(self.val_data)\n","\n","    @property\n","    def test(self):\n","        return self.make_dataset(self.tst_data)\n","    # Use [object].[Dataset_function].element_spec to review the structure of the Dataset\n","    # e.g.: std_window.train.element_spec =\n","    #   (TensorSpec(shape=(None, 6, 15), dtype=tf.float32, name=None),\n","    #    TensorSpec(shape=(None, 1, 1), dtype=tf.float32, name=None))\n","\n","    @property\n","    def example(self):\n","        # Get and cache an example batch of `inputs, labels` for plotting and asset investigation\n","        result = getattr(self, '_example', None)\n","        if result is None:\n","            # No example batch was found, so get one from the '.train' dataset\n","            result = next(iter(self.train))\n","            # And cache it for next time\n","            self._example = result\n","        return result\n","\n","    # Construct a function for viewing model outputs:\n","    def plot(self, model=None, plot_col=metals[0], max_subplots=3):\n","        # Pull a batch (std is *6*) of window values and save the input and label tensors\n","        inputs, labels = self.example\n","        # Generate a standard figure\n","        plt.figure(figsize=(12, 8))\n","        # Store the value of the label in the input column index (std is *14*)\n","        plot_col_index = self.column_indices[plot_col]\n","        # Plot subplots for each element in the batch (*6*) or max_sub, whichever is smaller\n","        max_n = min(max_subplots, len(inputs))\n","        # For each subplot:\n","        for n in range(max_n):\n","            plt.subplot(max_n, 1, n+1)\n","            # Each subplot will show real metal price\n","            plt.ylabel(f'{plot_col} Price [normed]')\n","            # Plot the price values for each of the training time steps (i.e., non-forecasted)\n","            plt.plot(self.input_indices, inputs[n, :, plot_col_index],\n","                     label='Inputs', marker='.', zorder=-10)\n","\n","            # The label index for a single list metal_label name is always 0\n","            if self.metal_label:\n","                label_col_index = 0\n","            else:\n","                label_col_index = plot_col_index\n","\n","            if label_col_index is None:\n","                continue\n","\n","            # If there is a label for the window, plot the labels (the actual values for each forecast)\n","            plt.scatter(self.label_indices, labels[n, :, label_col_index],\n","                        edgecolors='k', label='Labels', c='#2ca02c', s=64)\n","            # 'plot' works without a model (will ust show input and label prices); if there is a model, plot\n","            #  the predicted values for comparison with the label values (which will share an x-axis value)\n","            if model is not None:\n","                predictions = model(inputs)\n","                plt.scatter(self.label_indices, predictions[n, :, label_col_index],\n","                            marker='X', edgecolors='k', label='Predictions',\n","                            c='#ff7f0e', s=64)\n","\n","            if n == 0:\n","                plt.legend()\n","\n","        plt.xlabel('Month')\n","\n",""]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["\n","# Choose a metal to evaluate\n","metals = ['Aluminum']\n","\n","# Define a standard model window (1-month-ahead prediction from data up to six months behind)\n","std_window = SampleGenerator(metal_label=metals)\n","# Define a forecasting model window (6-month-ahead prediction from data up to six-months behind)\n","ahead_window = SampleGenerator(metal_label=metals, label_width=6, shift=6)\n","# Define appropriate window for baseline model (1 month back to predict one month forward)\n","single_window = SampleGenerator(metal_label=metals, input_width=1)\n","\n","# Display and confirm batch and input/label sizes\n","for example_inputs, example_labels in std_window.train.take(1):\n","    print(f'Inputs shape (batch, time, features): {example_inputs.shape}')\n","    print(f'Labels shape (batch, time, features): {example_labels.shape}')\n","column_names = pd.concat([econData, priceData[metals]], axis=1).columns\n","column_indices = {name: i for i, name in enumerate(column_names)}\n","\n","\n","# For ease in testing models later, define a function for testing separate models on separate windows\n","MAX_EPOCHS = 50\n","\n","\n","def compile_and_fit(model, window, patience=5):\n","    # Stop the model compiling if the value-loss parameter doesn't decrease at least once over two consecutive cycles\n","    early_stopping = tf.keras.callbacks.EarlyStopping(monitor='val_loss',   # monitor validation loss, vice training\n","                                                      patience=patience,\n","                                                      mode='min')\n","    # Compile model with standard loss and optimizer values\n","    model.compile(loss=tf.losses.MeanSquaredError(),\n","                  optimizer=tf.optimizers.Adam(),\n","                  metrics=[tf.metrics.MeanAbsoluteError()])\n","    # Output model fit data for storing and comparing\n","    history = model.fit(window.train, epochs=MAX_EPOCHS,\n","                        validation_data=window.validate,\n","                        callbacks=[early_stopping])\n","    return history\n","\n","\n","'''\n","The Time-Series Analysis (ARIMA modeling) previously performed, despite not otherwise having robust prediction quality,\n","provides insight into the underlying trend of the price data; that is, for each commodity, the principal trend is a\n","random walk.  Essentially, the commodity price for any particular month is the price from the previous month, altered\n","by a random fluctuation in with an experimentally-derived mean (not statistically different from 0) and standard\n","deviation.  Below is a baseline prediction algorithm that models this prediction behavior by predicting that the next\n","month's commodity price will be the previous month's commodity price; this is the baseline model.\n","'''\n","\n","# Define Baseline Model (need a special subclass of keras.Model)\n","\n","\n","class Baseline(tf.keras.Model):\n","    def __init__(self, label_index=None):\n","        # declare Baseline as a subclass of the tf.keras.Model class, inheriting functionality\n","        super().__init__()\n","        self.label_index = label_index\n","\n","    # '__call__' is a generic function for whenever you reference just the class name\n","    def call(self, inputs):\n","        if self.label_index is None:\n","            return inputs\n","        result = inputs[:, :, self.label_index]     # result.shape = (None, 1)\n","        # returnTensor.shape = (None, 1, 1)\n","        return result[:, :, tf.newaxis]\n","\n","\n","'''\n","Machine Learning models with multiple layers can be complex to the point where the interactions between variables and\n","their calculated weights are no longer understandable to even the experienced machine learning programmer.  This effect \n","can be positive since it allows more robust (and potentially overfit) predicted values better matching the actual values;\n","however, it does not aid in a simplistic understanding of each factor's role within the model.  The simplest model that\n","can be built is a linear regression model; this model is simple enough that extracting the weights for each factor at \n","each time step shows the correlated effect that factor has on the predicted price value.\n","'''\n","# Define a simple Linear model (this is used for discussing the role of each factor on the predicted price w/ 1 ts)\n","linear = tf.keras.Sequential([\n","    tf.keras.layers.Dense(units=1)\n","])\n","\n","'''\n","The documentation for the tf.keras.layer.dense class identifies that a 3-rank tensor fed into the layer will be 'shrunk',\n","in a sense, into a 2-rank tensor via the computation of the dot product between the inputs and the kernel along the last\n","axis.  This means that a (6,6,15) input tensor fed into the \n","'''\n","# Define a larger Linear model (this is used for discussing the role of each factor on the predicted price w/ 6 ts)\n","linMulti = tf.keras.Sequential([\n","    tf.keras.layers.Flatten(),\n","    tf.keras.layers.Dense(units=1),\n","    tf.keras.layers.Reshape([1, -1])\n","])\n","\n","# Define a Recurring Neural Network (RNN) model using the Long Short Term Memory (LSTM) Layer\n","lstm_model = tf.keras.models.Sequential([\n","    # Shape [6, 6, 15] => [6, 6, 30], because return_sequences is False, model does not predict for each time step\n","    tf.keras.layers.LSTM(30, return_sequences=False),\n","    # Shape => [6, 1, 1]\n","    tf.keras.layers.Dense(units=1)\n","])\n","\n","# Define a Recurring Neural Network (RNN) model for predicting up to 6 months out\n","lstm_ahead_model = tf.keras.models.Sequential([\n","    # Shape [6, 6, 15] => [6, 30], because return_sequences is true, model predicts for each time step\n","    tf.keras.layers.LSTM(30, return_sequences=True),\n","    # Shape => [6, 90]\n","    tf.keras.layers.Dense(units=6*15, kernel_initializer=tf.initializers.zeros()),\n","    # Shape => [6, 6, 15]\n","    tf.keras.layers.Reshape([6, 15])\n","])\n","\n","# Initiate value and performance dictionary to compare future models with baseline\n","val_performance = {}\n","performance = {}\n",""]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["\n","# Evaluate the Baseline models with TensorFlow/Keras performance indicators\n","basePricePred = Baseline(label_index=column_indices['Aluminum'])\n","\n","basePricePred.compile(loss=tf.losses.MeanSquaredError(),\n","                      metrics=[tf.metrics.MeanAbsoluteError()])\n","\n","# Save value\n","val_performance['Baseline'] = basePricePred.evaluate(single_window.validate)\n","performance['Baseline'] = basePricePred.evaluate(single_window.test, verbose=0)\n",""]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["history = compile_and_fit(linear, single_window)\n","# Save weight outputs as an Excel file (kernel is the weights matrix taken from the first/only layer)\n","single_weights = pd.Series(linear.layers[0].kernel[:, 0].numpy(),\n","                           index=column_names)\n","single_weights.to_excel(os.path.join(dwd, metals[0] + '_sing_weights.xlsx'))\n","\n","val_performance['Linear'] = linear.evaluate(single_window.validate)\n","performance['Linear'] = linear.evaluate(single_window.test, verbose=0)\n",""]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["history = compile_and_fit(linear, std_window)\n","print('Linear Summary with mult:\\n', linear.summary)\n","# Save weight outputs as an Excel file\n","# multi_weights = pd.DataFrame(linMulti.layers[1].kernel[:, 0].numpy().reshape((6, -1)),\n","#                              columns=column_names, index=[1, 2, 3, 4, 5, 6])\n","# multi_weights.to_excel(os.path.join(dwd, metals[0] + '_multi_weights.xlsx'))\n","\n","val_performance['LinMulti'] = linMulti.evaluate(std_window.validate)\n","performance['LinMulti'] = linMulti.evaluate(std_window.test, verbose=0)\n",""]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["history = compile_and_fit(lstm_model, std_window)\n","print('1-Step RNN Summary:\\n', lstm_model.summary)\n","\n","val_performance['RNN-1'] = lstm_model.evaluate(std_window.validate)\n","performance['RNN-1'] = lstm_model.evaluate(std_window.test, verbose=0)\n",""]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["history = compile_and_fit(lstm_ahead_model, ahead_window)\n","print('6-Step RNN Summary:\\n', lstm_ahead_model.summary)\n","\n","val_performance['RNN-6'] = lstm_ahead_model.evaluate(ahead_window.validate)\n","performance['RNN-6'] = lstm_ahead_model.evaluate(ahead_window.test, verbose=0)\n",""]}],"nbformat":4,"nbformat_minor":2,"metadata":{"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":3},"orig_nbformat":2}}